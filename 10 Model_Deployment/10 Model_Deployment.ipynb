{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make it clear that you're basically done with the entirety of the Pytorch Workflow (Except a few niche things from Pytorch 2.0)\n",
    "\n",
    "This part isn't very related to Pytorch workflow, it's to create an user interface so other people can actually use your machine learning model\n",
    "\n",
    "Since uh, we don't know how to do that, we are going to use some library to help us do it\n",
    "\n",
    "Gradio, the exact same interface tool as WebUI Stable Diffusion\n",
    "\n",
    "<img src=\"assets/Interface.png\" alt=\"The WorkFlow\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fancy name is called \"model deployment\", basically sending your model off into the real world and see how it performs out from this local notebook, there are a few things to consider though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where is the Model going to go?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, How much computing power does it need?\n",
    "\n",
    "- Running Locally on devices\n",
    "- Running on Cloud servers\n",
    "\n",
    "<img src=\"assets/Deployment_pro_con.png\" alt=\"The WorkFlow\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few examples of places where models go to\n",
    "\n",
    "<img src=\"assets/Tools_and_Places.png\" alt=\"The WorkFlow\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is the model going to function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, How long should it take the model run?\n",
    "\n",
    "- Running in Realtime\n",
    "- Running in Batch Processing\n",
    "\n",
    "``<img src=\"assets/Function_pro_con.png\" alt=\"The WorkFlow\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh, in this part he again trains 2 models, an EffNetB2 and a ViT\n",
    "\n",
    "I don't think it's necessary to create these models again, so I've just grabbed some previously trained ones from the before sections and start at the comparison between the 2 models (section 5.1)\n",
    "\n",
    "Which is going to be EffnetB0 from Transfer Learning, and ViT from Paper Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the big idea for comparing model performance is\n",
    "\n",
    "- speed\n",
    "- accuracy\n",
    "\n",
    "So we will iterate through all the data, see how accurate, and fast our EffnetB0 and ViT is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer \n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "def pred_and_store(model, transform, class_names, paths, device):\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        pred_dict = {}\n",
    "\n",
    "        #Get the sample path, and sample class name\n",
    "        pred_dict[\"image_path\"] = path\n",
    "        class_name = path.parent.stem\n",
    "        pred_dict[\"class_name\"] = class_name\n",
    "\n",
    "        #Start timer\n",
    "        start_time = timer()\n",
    "\n",
    "        #Open image path\n",
    "        image = Image.open(path)\n",
    "\n",
    "        #Transform the Image\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        #Set up model for use\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        #Sending it through the model\n",
    "        with torch.inference_mode():\n",
    "            prediction_logits = model(image)\n",
    "            prediction_probability_distribution = torch.softmax(prediction_logits, dim=1)\n",
    "            prediction_label = torch.argmax(prediction_probability_distribution, dim=1)  \n",
    "            prediction_class = class_names[prediction_label.cpu()] # The thing is, the list class_names is on the CPU and not GPU, so we have to send the label over to the CPU\n",
    "\n",
    "            pred_dict[\"pred_prob\"] = prediction_probability_distribution\n",
    "            pred_dict[\"pred_class\"] = prediction_class\n",
    "\n",
    "        #End the timer\n",
    "        end_time = timer()\n",
    "        pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n",
    "\n",
    "        #Does the pred match the true label?\n",
    "        pred_dict[\"correct\"] = class_name == prediction_class\n",
    "\n",
    "        #Add the dictionary to the list of predictions\n",
    "        pred_list.append(pred_dict)\n",
    "    \n",
    "    #Return list of prediction dictionaries\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first uh, we have to get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"Data/\")\n",
    "\n",
    "# Setup Dirs\n",
    "train_dir = data_path / \"train\"\n",
    "test_dir = data_path / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding all filepaths ending with '.jpg' in directory: Data\\test\n"
     ]
    }
   ],
   "source": [
    "# Get all test data paths\n",
    "print(f\"Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load EffnetB0 and run the predictions\n",
    "\n",
    "Though, this is actually quite tedious as we have modified the EffnetB0 by a bit, we need to make sure the model architecture matches the expected one by the saved weights\n",
    "\n",
    "So we need to:\n",
    "\n",
    "- Load a default pre-trained model from the architecture\n",
    "- Make our tweaks\n",
    "- Load our model from our pth file\n",
    "- Load the Dataloaders for EffnetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "#Load the Pretrained Model\n",
    "effnetb0_weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "effnetb0_transforms = effnetb0_weights.transforms()\n",
    "effnetb0 = torchvision.models.efficientnet_b0(weights=effnetb0_weights)\n",
    "\n",
    "#Update the classifier part of the model\n",
    "effnetb0.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=3, bias=True)).to(device)\n",
    "\n",
    "# Load your custom model saved weights\n",
    "model_path = 'models/pretrained_effnet_feature_extractor_pizza_steak_sushi.pth'\n",
    "effnetb0.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import data_setup\n",
    "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir, test_dir=test_dir, transform=effnetb0_transforms, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8e5d7bb2af461ea174055cb193253a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "effnetb0_test_pred_dicts = pred_and_store(model=effnetb0, transform=effnetb0_transforms, class_names=class_names, paths=test_data_paths, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ViT and run the predictions\n",
    "\n",
    "Yes, the same tedious procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "#Load the Pretrained Model\n",
    "vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "vit_transforms = vit_weights.transforms()\n",
    "vit = torchvision.models.vit_b_16(weights=vit_weights).to(device)\n",
    "\n",
    "#Update the classifier part of the model\n",
    "vit.heads = nn.Linear(in_features=768, out_features=3).to(device)\n",
    "\n",
    "# Load your custom model saved weights\n",
    "model_path = 'models/pretrained_vit_feature_extractor_pizza_steak_sushi.pth'\n",
    "vit.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import data_setup\n",
    "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir, test_dir=test_dir, transform=vit_transforms, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9caf2e66cae49009bdc3756caeb65f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vit_test_pred_dicts = pred_and_store(paths=test_data_paths, model=vit, transform=vit_transforms, class_names=class_names, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn into Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our comparison, we want to consider:\n",
    "\n",
    "- how accurate is the model in the test\n",
    "- how long did it take for the model in the test (average time per prediction)\n",
    "- how many parameters is the model\n",
    "- how large is the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effnetb0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>class_name</th>\n",
       "      <th>pred_prob</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>time_for_pred</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data\\test\\pizza\\1152100.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9272), tensor(0.0543), tensor(0.018...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data\\test\\pizza\\1503858.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.7563), tensor(0.0170), tensor(0.226...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data\\test\\pizza\\1687143.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.8898), tensor(0.0580), tensor(0.052...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data\\test\\pizza\\1925494.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.8118), tensor(0.0774), tensor(0.110...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data\\test\\pizza\\194643.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.6921), tensor(0.0580), tensor(0.249...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data\\test\\pizza\\195160.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.7784), tensor(0.1469), tensor(0.074...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data\\test\\pizza\\2003290.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.7350), tensor(0.2230), tensor(0.042...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data\\test\\pizza\\2019408.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.8516), tensor(0.0319), tensor(0.116...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data\\test\\pizza\\2111981.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9443), tensor(0.0114), tensor(0.044...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data\\test\\pizza\\2124579.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9897), tensor(0.0019), tensor(0.008...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image_path class_name  \\\n",
       "0  Data\\test\\pizza\\1152100.jpg      pizza   \n",
       "1  Data\\test\\pizza\\1503858.jpg      pizza   \n",
       "2  Data\\test\\pizza\\1687143.jpg      pizza   \n",
       "3  Data\\test\\pizza\\1925494.jpg      pizza   \n",
       "4   Data\\test\\pizza\\194643.jpg      pizza   \n",
       "5   Data\\test\\pizza\\195160.jpg      pizza   \n",
       "6  Data\\test\\pizza\\2003290.jpg      pizza   \n",
       "7  Data\\test\\pizza\\2019408.jpg      pizza   \n",
       "8  Data\\test\\pizza\\2111981.jpg      pizza   \n",
       "9  Data\\test\\pizza\\2124579.jpg      pizza   \n",
       "\n",
       "                                           pred_prob pred_class  \\\n",
       "0  [[tensor(0.9272), tensor(0.0543), tensor(0.018...      pizza   \n",
       "1  [[tensor(0.7563), tensor(0.0170), tensor(0.226...      pizza   \n",
       "2  [[tensor(0.8898), tensor(0.0580), tensor(0.052...      pizza   \n",
       "3  [[tensor(0.8118), tensor(0.0774), tensor(0.110...      pizza   \n",
       "4  [[tensor(0.6921), tensor(0.0580), tensor(0.249...      pizza   \n",
       "5  [[tensor(0.7784), tensor(0.1469), tensor(0.074...      pizza   \n",
       "6  [[tensor(0.7350), tensor(0.2230), tensor(0.042...      pizza   \n",
       "7  [[tensor(0.8516), tensor(0.0319), tensor(0.116...      pizza   \n",
       "8  [[tensor(0.9443), tensor(0.0114), tensor(0.044...      pizza   \n",
       "9  [[tensor(0.9897), tensor(0.0019), tensor(0.008...      pizza   \n",
       "\n",
       "   time_for_pred  correct  \n",
       "0         0.1068     True  \n",
       "1         0.0391     True  \n",
       "2         0.0335     True  \n",
       "3         0.0356     True  \n",
       "4         0.0329     True  \n",
       "5         0.0305     True  \n",
       "6         0.0311     True  \n",
       "7         0.0314     True  \n",
       "8         0.0604     True  \n",
       "9         0.0302     True  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the test_pred_dicts into a DataFrame\n",
    "import pandas as pd\n",
    "effnetb0_test_pred_df = pd.DataFrame(effnetb0_test_pred_dicts)\n",
    "effnetb0_test_pred_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of correct predictions\n",
    "effnetb0_results = effnetb0_test_pred_df.correct.value_counts()\n",
    "effnetb0_results_dict = effnetb0_results.to_dict()\n",
    "\n",
    "# Find the average time per prediction \n",
    "effnetb0_average_time_per_pred = round(effnetb0_test_pred_df.time_for_pred.mean(), 4)\n",
    "\n",
    "# Count number of parameters in EffNetB2\n",
    "effnetb0_total_params = sum(torch.numel(param) for param in effnetb0.parameters())\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "pretrained_effnetb0_model_size = Path(\"models/pretrained_effnet_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with EffNetB2 statistics\n",
    "effnetb2_stats = {\"number_of_parameters\": effnetb0_total_params,\n",
    "                  \"model_size (MB)\": pretrained_effnetb0_model_size,\n",
    "                  \"pred_correct_results\": effnetb0_results_dict.get(True, 0),\n",
    "                  \"pred_wrong_results\": effnetb0_results_dict.get(False, 0),\n",
    "                  \"time_per_pred_cpu\": effnetb0_average_time_per_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>class_name</th>\n",
       "      <th>pred_prob</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>time_for_pred</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data\\test\\pizza\\1152100.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9949), tensor(0.0020), tensor(0.003...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data\\test\\pizza\\1503858.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9927), tensor(0.0020), tensor(0.005...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.1228</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data\\test\\pizza\\1687143.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9974), tensor(0.0016), tensor(0.001...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data\\test\\pizza\\1925494.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9902), tensor(0.0029), tensor(0.006...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0870</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data\\test\\pizza\\194643.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9970), tensor(0.0020), tensor(0.001...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data\\test\\pizza\\195160.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9985), tensor(0.0010), tensor(0.000...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0861</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data\\test\\pizza\\2003290.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9978), tensor(0.0011), tensor(0.001...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data\\test\\pizza\\2019408.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9946), tensor(0.0035), tensor(0.001...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data\\test\\pizza\\2111981.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9974), tensor(0.0012), tensor(0.001...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data\\test\\pizza\\2124579.jpg</td>\n",
       "      <td>pizza</td>\n",
       "      <td>[[tensor(0.9951), tensor(0.0027), tensor(0.002...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image_path class_name  \\\n",
       "0  Data\\test\\pizza\\1152100.jpg      pizza   \n",
       "1  Data\\test\\pizza\\1503858.jpg      pizza   \n",
       "2  Data\\test\\pizza\\1687143.jpg      pizza   \n",
       "3  Data\\test\\pizza\\1925494.jpg      pizza   \n",
       "4   Data\\test\\pizza\\194643.jpg      pizza   \n",
       "5   Data\\test\\pizza\\195160.jpg      pizza   \n",
       "6  Data\\test\\pizza\\2003290.jpg      pizza   \n",
       "7  Data\\test\\pizza\\2019408.jpg      pizza   \n",
       "8  Data\\test\\pizza\\2111981.jpg      pizza   \n",
       "9  Data\\test\\pizza\\2124579.jpg      pizza   \n",
       "\n",
       "                                           pred_prob pred_class  \\\n",
       "0  [[tensor(0.9949), tensor(0.0020), tensor(0.003...      pizza   \n",
       "1  [[tensor(0.9927), tensor(0.0020), tensor(0.005...      pizza   \n",
       "2  [[tensor(0.9974), tensor(0.0016), tensor(0.001...      pizza   \n",
       "3  [[tensor(0.9902), tensor(0.0029), tensor(0.006...      pizza   \n",
       "4  [[tensor(0.9970), tensor(0.0020), tensor(0.001...      pizza   \n",
       "5  [[tensor(0.9985), tensor(0.0010), tensor(0.000...      pizza   \n",
       "6  [[tensor(0.9978), tensor(0.0011), tensor(0.001...      pizza   \n",
       "7  [[tensor(0.9946), tensor(0.0035), tensor(0.001...      pizza   \n",
       "8  [[tensor(0.9974), tensor(0.0012), tensor(0.001...      pizza   \n",
       "9  [[tensor(0.9951), tensor(0.0027), tensor(0.002...      pizza   \n",
       "\n",
       "   time_for_pred  correct  \n",
       "0         0.4680     True  \n",
       "1         0.1228     True  \n",
       "2         0.0852     True  \n",
       "3         0.0870     True  \n",
       "4         0.0873     True  \n",
       "5         0.0861     True  \n",
       "6         0.1099     True  \n",
       "7         0.0930     True  \n",
       "8         0.0899     True  \n",
       "9         0.1220     True  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn vit_test_pred_dicts into a DataFrame\n",
    "import pandas as pd\n",
    "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
    "vit_test_pred_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of correct predictions\n",
    "vit_results = vit_test_pred_df.correct.value_counts()\n",
    "vit_results_dict = vit_results.to_dict()\n",
    "\n",
    "# Find the average time per prediction \n",
    "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
    "\n",
    "# Count number of parameters in EffNetB2\n",
    "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "pretrained_vit_model_size = Path(\"models/pretrained_effnet_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with EffNetB2 statistics\n",
    "vit_stats = {\"number_of_parameters\": vit_total_params,\n",
    "            \"model_size (MB)\": pretrained_vit_model_size,\n",
    "            \"pred_correct_results\": vit_results_dict.get(True, 0),\n",
    "            \"pred_wrong_results\": vit_results_dict.get(False, 0),\n",
    "            \"time_per_pred_cpu\": vit_average_time_per_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a new dataframe from our statistics dictionaries, so this comparison can look a lot more straight forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>number_of_parameters</th>\n",
       "      <th>model_size (MB)</th>\n",
       "      <th>pred_correct_results</th>\n",
       "      <th>pred_wrong_results</th>\n",
       "      <th>time_per_pred_cpu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EffNetB2</td>\n",
       "      <td>4011391</td>\n",
       "      <td>15</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ViT</td>\n",
       "      <td>85800963</td>\n",
       "      <td>15</td>\n",
       "      <td>69</td>\n",
       "      <td>6</td>\n",
       "      <td>0.1109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  number_of_parameters  model_size (MB)  pred_correct_results  \\\n",
       "0  EffNetB2               4011391               15                    65   \n",
       "1       ViT              85800963               15                    69   \n",
       "\n",
       "   pred_wrong_results  time_per_pred_cpu  \n",
       "0                  10             0.0354  \n",
       "1                   6             0.1109  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn stat dictionaries into DataFrame\n",
    "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
    "\n",
    "# Add column for model names\n",
    "df[\"model\"] = [\"EffNetB0\", \"ViT\"]\n",
    "\n",
    "# get a list of columns\n",
    "cols = list(df)\n",
    "\n",
    "# move the column to head of list using index, pop and insert\n",
    "cols.insert(0, cols.pop(cols.index(\"model\")))\n",
    "df = df.loc[:, cols]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some takeaways are:\n",
    "\n",
    "- ViT is a lot larger, and more accurate than EffNetB0\n",
    "- On Average, EffnetB0 is a lot faster than Vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment with Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, we've choosen EffnetB0 for it's faster performance, and now we are going to deploy it using Gradio \n",
    "\n",
    "From their own words <b>\"Gradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere!\"</b>\n",
    "\n",
    "So many libraries, just... so many\n",
    "\n",
    "<img src=\"assets/Gradio.png\" alt=\"The WorkFlow\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be what we will create towards the end\n",
    "\n",
    "<img src=\"assets/FoodVisionMini.png\" alt=\"The WorkFlow\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way Gradio creates an interface is by using the function gradio.Interface(fn, inputs, outputs):\n",
    "\n",
    "Where, fn is a Python function to map the inputs to the outputs\n",
    "\n",
    "So we need to create an interface that allows inputs -> ML model -> outputs\n",
    "\n",
    "For this use case, we have images of food -> EffNetB2 -> prediction of food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will be a similar function to pred_and_store(), but we just need to make a prediction on a single image with EffNetB0\n",
    "\n",
    "input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put EffNetB0 on CPU, as running it on a GPU in hugging face costs money\n",
    "effnetb0.to(\"cpu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "def effnetb0_predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"\n",
    "    Transforms and performs a prediction on img and returns prediction and time taken.\n",
    "    \"\"\"\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the target image and add a batch dimension\n",
    "    img = effnetb0_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put model into evaluation mode and turn on inference mode\n",
    "    effnetb0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(effnetb0(img), dim=1)\n",
    "\n",
    "    # Initialize an empty dictionary\n",
    "    pred_labels_and_probs = {}\n",
    "\n",
    "    # Loop over the indices of class_names, and create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    for i in range(len(class_names)):\n",
    "        # Use the class name as the key and the corresponding predicted probability as the value\n",
    "        pred_labels_and_probs[class_names[i]] = float(pred_probs[0][i])\n",
    "\n",
    "    \n",
    "    # Calculate the prediction time\n",
    "    end_time = timer()\n",
    "    pred_time = round(end_time - start_time, 4)\n",
    "    \n",
    "    # Return the prediction dictionary and prediction time \n",
    "    return pred_labels_and_probs, pred_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Examples of Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of example images, so people can directly try the model out with these images to see how our model will perform on food images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Data\\\\test\\\\sushi\\\\1600999.jpg'],\n",
       " ['Data\\\\test\\\\pizza\\\\1503858.jpg'],\n",
       " ['Data\\\\test\\\\steak\\\\1285886.jpg']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# So each time we run this we get the same results\n",
    "random.seed(25)\n",
    "\n",
    "# Create a list of example inputs to our Gradio demo\n",
    "examples_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
    "examples_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Gradio Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to input the following parameters to the gradio.Interface() class:\n",
    "\n",
    "- fn: a function that maps inputs to outputs, in this case `effnetb0_predict`\n",
    "- inputs: the input to the interface which get passed to the function, in this case `image`\n",
    "- outputs: the output of the function which gets passed to the interface, in this case `label` and `time taken` (a number)\n",
    "- examples: a list a of examples to showcase the model\n",
    "- title: a title of the model\n",
    "- description: a description of the model\n",
    "- article: a reference note of the model\n",
    "\n",
    "This will return a local url which you can open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio\n",
    "\n",
    "title = \"FoodVision Mini üçïü•©üç£\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
    "article = \"Created at (https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
    "\n",
    "demo = gradio.Interface(fn=effnetb0_predict, \n",
    "                        inputs=\"image\", \n",
    "                        outputs=[gradio.Label(num_top_classes=3, label=\"Predictions\"), gradio.Number(label=\"Prediction time (s)\")],\n",
    "                        examples=examples_list, \n",
    "                        title=title,\n",
    "                        description=description,\n",
    "                        article=article)\n",
    "\n",
    "# Launch the demo!\n",
    "demo.launch(debug=False, # print errors locally?\n",
    "            share=False) # generate a publicly shareable URL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a random image found online of a picture of steak, let's see if our model get's it correct\n",
    "\n",
    "<img src=\"assets/Juicy_Steak.jpg\" alt=\"The WorkFlow\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes it does, with a 68% guess of steak\n",
    "\n",
    "<img src=\"assets/Gradio_Example.png\" alt=\"The WorkFlow\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, our machine learning models can be used by anyone through just a simple interface without any touching of code\n",
    "\n",
    "Of course, there's more detail to dive into for how you want specific layouts to look like etc in Gradio, but that you can do so in your own time for your own projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Hugging Face Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can imagine this part as \"the model is done, let's throw it to a platform where everyone can see, use, and download\"\n",
    "\n",
    "Hugging Face is basically the equivalent of Github for machine learning (It tries to be)\n",
    "\n",
    "If having a good GitHub portfolio showcases your coding abilities, having a good Hugging Face portfolio can showcase your machine learning abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>There are many other places we could upload and host our Gradio app such as, Google Cloud, AWS (Amazon Web Services) or other cloud vendors\n",
    "\n",
    "however, we're going to use Hugging Face Spaces due to the ease of use and wide adoption by the machine learning community.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything must go under a single directory, this is an example of the final structure\n",
    "\n",
    "```\n",
    "demos/\n",
    "‚îî‚îÄ‚îÄ foodvision_mini/\n",
    "    ‚îú‚îÄ‚îÄ pretrained_effnet_feature_extractor_pizza_steak_sushi.pth\n",
    "    ‚îú‚îÄ‚îÄ app.py\n",
    "    ‚îú‚îÄ‚îÄ examples/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ example_1.jpg\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ example_2.jpg\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ example_3.jpg\n",
    "    ‚îú‚îÄ‚îÄ model.py\n",
    "    ‚îî‚îÄ‚îÄ requirements.txt\n",
    "```\n",
    "\n",
    "- pretrained_effnet_feature_extractor is our trained Pytorch Model\n",
    "- app.py is the gradio app\n",
    "- examples contain example images for the gradio app\n",
    "- model.py is the definition of our model, transforms, etc\n",
    "- requirements.txt explains the dependencies to run our app such as torch, torchvision and gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Structure in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create demos folder, and foodvision_mini folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "foodvision_mini_path = Path(\"demos/foodvision_mini\")\n",
    "foodvision_mini_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating examples folder, and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an examples directory\n",
    "examples_path = foodvision_mini_path / \"examples\"\n",
    "examples_path.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "# Collect three random test dataset image paths\n",
    "example_list_path = [Path(path_string[0]) for path_string in examples_list]\n",
    "\n",
    "# Copy the three random images to the examples directorys\n",
    "for example_path in example_list_path:\n",
    "    destination_path = examples_path / example_path.name\n",
    "    shutil.copy2(src=example_path, dst=destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying effnetb0 over to demos folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a source path for our target model\n",
    "effnetb0_path = \"models/pretrained_effnet_feature_extractor_pizza_steak_sushi.pth\"\n",
    "\n",
    "# Create a destination path for our target model\n",
    "effnetb0_destination_path = foodvision_mini_path /  \"pretrained_effnet_feature_extractor_pizza_steak_sushi.pth\"\n",
    "\n",
    "# Copy the model over\n",
    "shutil.copy2(src=effnetb0_path, dst=effnetb0_destination_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need a way to instantiate a model, basically load the architecture and transforms\n",
    "\n",
    "create that and put it as a python script, model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demos/foodvision_mini/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/foodvision_mini/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_effnetb0_model(seed:int=42, \n",
    "                          num_classes:int=3):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates an EfficientNetB0 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNet0 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB0 image transforms.\n",
    "    \"\"\"\n",
    "\n",
    "    #Load the Pretrained Model\n",
    "    effnetb0_weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    effnetb0_transforms = effnetb0_weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b0(weights=effnetb0_weights)\n",
    "\n",
    "    #Update the classifier part of the model\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features=1280, out_features=num_classes, bias=True))\n",
    "\n",
    "    #Freeze all layers as we are not training it (this speeds up computation)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return model, effnetb0_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, creating app.py, which loads our model from saved weights, and creates a Gradio Interface\n",
    "\n",
    "We call it app.py because by default when you create a HuggingFace Space, it looks for a file called app.py to run and host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demos/foodvision_mini/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/foodvision_mini/app.py\n",
    "\n",
    "\n",
    "\n",
    "### 1. Imports and class names setup ### \n",
    "\n",
    "import gradio\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_effnetb0_model\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "\n",
    "\n",
    "### 2. Model and transforms preparation ###\n",
    "\n",
    "# Setup class names\n",
    "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
    "\n",
    "# Create EffNetB2 model\n",
    "effnetb0, effnetb0_transforms = create_effnetb0_model(num_classes=len(class_names))\n",
    "\n",
    "# Load saved weights to the CPU\n",
    "effnetb0.load_state_dict(torch.load(f=\"pretrained_effnet_feature_extractor_pizza_steak_sushi.pth\", map_location=torch.device(\"cpu\"),))\n",
    "\n",
    "\n",
    "\n",
    "### 3. Predict function ###\n",
    "\n",
    "# Create predict function\n",
    "def effnetb0_predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"\n",
    "    Transforms and performs a prediction on img and returns prediction and time taken.\n",
    "    \"\"\"\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the target image and add a batch dimension\n",
    "    img = effnetb0_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put model into evaluation mode and turn on inference mode\n",
    "    effnetb0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(effnetb0(img), dim=1)\n",
    "\n",
    "    # Initialize an empty dictionary\n",
    "    pred_labels_and_probs = {}\n",
    "\n",
    "    # Loop over the indices of class_names, and create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    for i in range(len(class_names)):\n",
    "        # Use the class name as the key and the corresponding predicted probability as the value\n",
    "        pred_labels_and_probs[class_names[i]] = float(pred_probs[0][i])\n",
    "\n",
    "    \n",
    "    # Calculate the prediction time\n",
    "    end_time = timer()\n",
    "    pred_time = round(end_time - start_time, 4)\n",
    "    \n",
    "    # Return the prediction dictionary and prediction time \n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "\n",
    "\n",
    "### 4. Gradio app ###\n",
    "\n",
    "# Create title, description and article strings\n",
    "title = \"FoodVision Mini üçïü•©üç£\"\n",
    "description = \"An EfficientNetB0 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
    "article = \"Created at (https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
    "\n",
    "# Create examples list from \"examples/\" directory\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "demo = gradio.Interface(fn=effnetb0_predict, \n",
    "                        inputs=\"image\", \n",
    "                        outputs=[gradio.Label(num_top_classes=3, label=\"Predictions\"), gradio.Number(label=\"Prediction time (s)\")],\n",
    "                        examples=example_list, \n",
    "                        title=title,\n",
    "                        description=description,\n",
    "                        article=article)\n",
    "\n",
    "# Launch the demo!\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a minimum_requirements.txt file, basically what you need to run this machine learning model demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demos/foodvision_mini/minimum_requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/foodvision_mini/minimum_requirements.txt\n",
    "torch==1.12.0\n",
    "torchvision==0.13.0\n",
    "gradio==3.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "packaging it all up into a zip for upload\n",
    "\n",
    "- base_name is the final name\n",
    "- format is the format which we want the archive to be in\n",
    "- root_dir is the directory which we create the archive\n",
    "- base_dir is the directory which we output the archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\1. Python (AI)\\\\1. Tutorials\\\\4. Machine Learning Libraries\\\\1. Pytorch\\\\10. Model_Deployment\\\\../data/demos_ready_for_hugging_face.zip'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(base_name=\"demos_ready_for_hugging_face\", format=\"zip\", root_dir=\"demos\", base_dir=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obviously, we won't really do this, but we will still see some ways on how to do so\n",
    "\n",
    "- Uploading using Hugging Face Hub python library\n",
    "- Uploading via the Hugging Face Web interface\n",
    "- Upload via Command Line of Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then he goes into the specifics of creating an account, selecting a bunch of things, how to do Git on Command Line... I haven't touched a single piece of git yet\n",
    "\n",
    "So nah  ¬Ø \\ _ („ÉÑ) _ / ¬Ø , we'll call it a day here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, we will see an embedded, online FoodVision Mini Gradio demo into our notebook as an iframe with IPython.display.IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1300\"\n",
       "            height=\"800\"\n",
       "            src=\"https://hf.space/embed/mrdbourke/foodvision_mini/+\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x22e1d2458a0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IPython is a library to help make Python interactive\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Embed FoodVision Mini Gradio demo\n",
    "IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_mini/+\", width=1300, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Vision Big?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He then proceeds to build a BIG Food Vision Model and uploads it to hugging face, which predicts from not 3 classes, but 101\n",
    "\n",
    "Uh... no thank you again, I really feel fine stopping here, and moving on to my own projects\n",
    "\n",
    "which, this is the end of pytorch's basics (except addition niche things in pytorch 2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
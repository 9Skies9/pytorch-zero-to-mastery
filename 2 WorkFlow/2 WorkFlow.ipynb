{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WorkFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Workflow.png\" alt=\"The WorkFlow\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn gives us the fundemental building blocks  (layers) for neural networks\n",
    "\n",
    "1.  Prepare and load data\n",
    "2.  Making Model\n",
    "3.  Traning Model\n",
    "4.  Evaluating Model\n",
    "5.  Saving/Loading Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a neural network for linear regression.\n",
    "\n",
    "Basically a fancy way to say letting the nn guess the next few numbers that will come in a linear line when we give it some data of points of a linear line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can almost be anything, as long it can be put into a numerical representation\n",
    "\n",
    "*   Spread Sheet\n",
    "*   Images\n",
    "*   Videos\n",
    "*   Audios\n",
    "*   Text\n",
    "*   And More...\n",
    "\n",
    "So then we can build a model to find patterns in those numerical representations, and generalize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we will give datapoints on a linear line (y = ax + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a linear line, but with a weight and a bias\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "#Creating x values list\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "x = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = weight * x + bias\n",
    "\n",
    "x[0:10], y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data that has been created will be divided into 2 datasets, training and testing data sets.\n",
    "\n",
    "As for what they do... it's rather obvious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a ratio split of training/testing set sample size\n",
    "#so everything up to [split] inside both lists is in the training list, everything after is in the testing list\n",
    "\n",
    "split = int(len(x) * 0.8)\n",
    "x_train, y_train = x[:split], y[:split]\n",
    "x_test, y_test = x[split:], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing data using matplotlib\n",
    "\n",
    "def plot_predictions(train_x=x_train, train_y=y_train, test_x=x_test, test_y=y_test, predictions=None):\n",
    "  plt.figure(figsize=(10, 7))\n",
    "\n",
    "  #plot training data\n",
    "  plt.scatter(train_x, train_y, c=\"b\", s=4, label=\"Training Data\")\n",
    "\n",
    "  #plot testing data (in the sameplot)\n",
    "  plt.scatter(test_x, test_y, c=\"g\", s=4, label=\"Testing Data\")\n",
    "\n",
    "  #when we want to compare predictions with testing\n",
    "  if predictions != None:\n",
    "    plt.scatter(test_x, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "\n",
    "  #makes a legend\n",
    "  plt.legend(prop={\"size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a python class for the model\n",
    "#This takes in nn.module as a super class, which allows our class to access all the functionalities of our super class\n",
    "class LinearRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        #Initialize the super class\n",
    "        super().__init__()\n",
    "\n",
    "        #Randomization of variables, parameter allows us to do some special things (storing tensor data, tracking gradients for performing gradient descent, and assigning the data types)\n",
    "        self.weights = torch.nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "\n",
    "\n",
    "    #There is a built in \"forward\"method in the nn.Module\n",
    "    #But we overwrote it with this as the default forward computation method for the model (y = ax+b)\n",
    "    def forward(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        return self.weights * values + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Model Building Essentials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the model do?\n",
    "\n",
    "1.  Start with ramdom values of weight and bias\n",
    "2.  Adjust those random values to perform better on training data\n",
    "\n",
    "\n",
    "How does it do that?\n",
    "\n",
    "1.   Backpropagation\n",
    "2.   Graident Descent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheatsheet for nn building moduels (really check this out)\n",
    "https://pytorch.org/tutorials/beginner/ptcheat.html?highlight=cheat\n",
    "\n",
    "\n",
    "\n",
    "*   torch.nn - contains all the building blocks of the nn\n",
    "*   torch.nn.Paremeter - the paremeters for pytorch layers\n",
    "*   torch.nn.Module - the functions are all written here\n",
    "*   torch.optim - all the algorithms for gradient descent is here\n",
    "*   def foward() - you have to hand define this and overwrite the one in nn.Module, this defines what happends in forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the model's internals using \"paremeters()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our model is really simple, so there isn't a lot that we can peek inside\n",
    "\n",
    "\n",
    "#Creating a random seed allows us to get the same values from the random functions\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#Create a object of our model calss\n",
    "cool_model = LinearRegressionModel()\n",
    "\n",
    "#Checking the paremeters in a list (because or else we cannot see the paremeters properly)\n",
    "list(cool_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking out the paremeters named in a list with state_dict()\n",
    "cool_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also running the initial network's prediction once so down at seeing results we can see some differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions_original = cool_model(x_test)\n",
    "y_predictions_original = y_predictions_original.detach().numpy()\n",
    "y_predictions_original = y_predictions_original.tolist()\n",
    "y_predictions_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 modes for a model, training, evaluation, and infrance.\n",
    "\n",
    "* Training is where we tweak the paremeters for a better performing network\n",
    "* Evaluation is... evaluating how good the network is, not updating the paremeters\n",
    "* Infrance is using the model to make predictions on new data, basically putting the network to real world use\n",
    "\n",
    "Basically, infrance mode will stop pytorch from doing a few more things behind the scenes and make the code run quicker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... the fun of math kicks in here, you know what's behind the scenes\n",
    "\n",
    "Cost/Loss functions, Optimizer/Gradient Descent... yada yada\n",
    "Do be aware that different scenarios will be better off using different loss functions and optimizers, search them later\n",
    "\n",
    "* https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "* https://pytorch.org/docs/stable/optim.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are using the mean absoulte value difference loss\n",
    "loss_function = torch.nn.L1Loss()\n",
    "\n",
    "#We are using Stochasitic Gradient Descent on our model's paremeters, alongside a learning rate\n",
    "optimizer = torch.optim.SGD(params=cool_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the demonstration video, he wrote the training and testing together\n",
    "\n",
    "but we'll write a def here for the testing and throw it in the training loop above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_predictions = model(x_test)\n",
    "    test_loss = loss_function(test_predictions, y_test)\n",
    "\n",
    "    return test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Testing Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what should happen in a training loop\n",
    "\n",
    "0. Get Data\n",
    "1. Forward Propagation\n",
    "2. Loss Calculation\n",
    "3. Initialize Gradient Arrays\n",
    "4. Back Propagation\n",
    "5. Update Parameters\n",
    "6. Repeat (Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many steps we doing\n",
    "epochs = 160\n",
    "\n",
    "epoch_count = []\n",
    "training_loss_values = []\n",
    "testing_loss_values = []\n",
    "\n",
    "\n",
    "#Repeat (Gradient Descent)\n",
    "for epoch in range(epochs):\n",
    "    epoch_count.append(epoch)\n",
    "\n",
    "    #Set model to training mode\n",
    "    cool_model.train()\n",
    "\n",
    "    #Get data, and forward propagation\n",
    "    y_predictions = cool_model(x_train)\n",
    "\n",
    "    #Loss calculation\n",
    "    train_loss = loss_function(y_predictions, y_train)\n",
    "    training_loss_values.append(train_loss)\n",
    "\n",
    "    #Initialize Gradient Arrays... this means we are creating empty arrays to hold all the components of the gradient for each parameter in the nn.\n",
    "    #So then we can perform a step in gradient descent, using corrrespinding components of the gradient in the now filled arrays to update the weights biases.\n",
    "    #In pytorch, apparently this is already done, but we have to do zero_grad(), to clear out the accumalated gradients by back propagation\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #Back propagation\n",
    "    train_loss.backward()\n",
    "\n",
    "    #Update Parameters\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    #Testing Inside Training Loop\n",
    "    \n",
    "    #Set mode to evaluation mode\n",
    "    cool_model.eval()\n",
    "\n",
    "    #testing\n",
    "    test_loss = testing(cool_model)\n",
    "    testing_loss_values.append(test_loss)\n",
    "\n",
    "    #Printing out model's parameters\n",
    "    print(cool_model.state_dict())\n",
    "    print(f\"epoch {epoch}, training_loss {train_loss}, testing_loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original values with the seed of 42\n",
    "* weight = 0.3367\n",
    "* bias = 0.1288\n",
    "\n",
    "The desired values\n",
    "* weight = 0.7\n",
    "* bias = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original prediction\n",
    "plot_predictions(predictions=y_predictions_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final prediction\n",
    "with torch.inference_mode():\n",
    "    y_predictions_new = cool_model(x_test)\n",
    "plot_predictions(predictions=y_predictions_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting changes in loss over epoches\n",
    "\n",
    "#Turning list of pytorch tensor data into numpy arrays for matplotlib\n",
    "training_loss_values = np.array(torch.tensor(training_loss_values).detach().numpy())\n",
    "testing_loss_values = np.array(torch.tensor(testing_loss_values).detach().numpy())\n",
    "\n",
    "#The plot for real\n",
    "plt.plot(epoch_count, training_loss_values, label=\"Training Loss\")\n",
    "plt.plot(epoch_count, testing_loss_values, label=\"Testing Loss\")\n",
    "plt.title(\"Training and Testing Loss Curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickles!\n",
    "\n",
    "1. `torch.save()` allows us to save a pytorch object in pickle format\n",
    "2. `torch.load()` allows us to load to pytorch object, from pickle format\n",
    "3. `torch.nn.Module.load_state_dict()` loads a model's state dictionary, basically a python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\1. Coding\\1. Tutorials\\Python\\3. Libraries\\5. Pytorch\\2. WorkFlow.ipynb Cell 45\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/1.%20Coding/1.%20Tutorials/Python/3.%20Libraries/5.%20Pytorch/2.%20WorkFlow.ipynb#X62sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model_save_path \u001b[39m=\u001b[39m model_path \u001b[39m/\u001b[39m model_name\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/1.%20Coding/1.%20Tutorials/Python/3.%20Libraries/5.%20Pytorch/2.%20WorkFlow.ipynb#X62sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#Saving the state dict\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/1.%20Coding/1.%20Tutorials/Python/3.%20Libraries/5.%20Pytorch/2.%20WorkFlow.ipynb#X62sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m torch\u001b[39m.\u001b[39msave(obj\u001b[39m=\u001b[39mcool_model\u001b[39m.\u001b[39mstate_dict(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/1.%20Coding/1.%20Tutorials/Python/3.%20Libraries/5.%20Pytorch/2.%20WorkFlow.ipynb#X62sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m            f\u001b[39m=\u001b[39mmodel_save_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "#We can use os.path or pathlib.Path for saving\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#Create directory\n",
    "model_path = Path(\"models\")\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#Create saving path, usually pytorch files are called \"pth\"\n",
    "model_name = \"pytorch_workflow_model_0.pth\"\n",
    "model_save_path = model_path / model_name\n",
    "\n",
    "#Saving the state dict\n",
    "torch.save(obj=cool_model.state_dict(),\n",
    "           f=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll need to create a new model and load the saved state_dict() into the new model\n",
    "cooler_model = LinearRegressionModel()\n",
    "\n",
    "#loading the saved state dict from the new model, with torch.load()\n",
    "cooler_model.load_state_dict(torch.load(f=model_save_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
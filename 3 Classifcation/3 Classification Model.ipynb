{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 types of classification that exists\n",
    "1. Binary classification (Yes or No)\n",
    "2. Multi-class classification (Multiple Choice Question)\n",
    "3. Multi-label classification (Multiple Answers for a Question)\n",
    "\n",
    "The example here is using scikit learn to create a dataset of samples that are either grouped into circle 1 or 2.  \n",
    "The model we make, when given a sample needs to predict if it is on circle 1 or 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using scikit learn, make data points either on circle 1 or 2\n",
    "#(x, y) = (data point tuple, label for which circle)\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "number_of_samples = 1000\n",
    "x, y = make_circles(n_samples=number_of_samples, noise=0.03, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas, make data frame of data (?)\n",
    "#This syntax of \":\" is slicing for numpy, without specifying \"start_index : end_index : step\" it just means include all rows\n",
    "#So for example, x[:,0] means from all rows, select the \"0\" index element for each row.\n",
    "\n",
    "circles = pd.DataFrame({\"x1\": x[:,0], \"x2\": x[:,1], \"label\": y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using matplotlib, visualization of data\n",
    "plt.scatter(x=x[:,0], y=x[:,1], c=y, cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using pytorch, turning data into tensors\n",
    "x = torch.from_numpy(x).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using scikit learn, randomly distribute data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking length of our samples\n",
    "len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lv1 Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup device agnistic code to run code on GPU\n",
    "2. Making Class using nn.Module\n",
    "3. Define loss function and optimizier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Device Agnostic Code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Class by nn.Module\n",
    "class Classification_lv1(nn.Module):\n",
    "\n",
    "    #network layers\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #In this case our input (x) has shape [2], output (y) has shape [1]\n",
    "        #So we have made a network of [2, 5, 1] neurons per layer\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5)\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "    #forward propagation\n",
    "    #by default, pytorch already made you one, we're overwriting it for the convenience of this case\n",
    "    def forward(self, x):\n",
    "        return self.layer_2(self.layer_1(x)) #x -> layer_1 -> layer_2\n",
    "    \n",
    "\n",
    "    #Gives back accuracy of training data\n",
    "    def accuracy(self, predictions, labeled_data):\n",
    "        correct = torch.eq(predictions, labeled_data).sum().item()\n",
    "        total_samples = len(labeled_data)\n",
    "        percentage_correct = (correct/total_samples) * 100\n",
    "        return percentage_correct\n",
    "    \n",
    "\n",
    "\n",
    "#Throw model to target device\n",
    "Model_Classification = Classification_lv1().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #3. Define Loss Function and Optimizer inside init method\n",
    "\n",
    "#This is binary cross entropy + sigmoid activations\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#Good old stochastic gradient descent\n",
    "optimizer = torch.optim.SGD(params=Model_Classification.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training/Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply a sigmoid activation to squish our outputs between 0 and 1, then a round function to turn the output into either 1 or 0 (on this circle or that circle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "#Sending to cuda device\n",
    "x_test, x_train, y_test, y_train = x_test.to(device), x_train.to(device), y_test.to(device), y_train.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\n",
    "    #Training\n",
    "    Model_Classification.train()\n",
    "\n",
    "    #Round the output predictions\n",
    "    y_predictions_s = Model_Classification(x_train).squeeze()\n",
    "    y_predictions = torch.round(torch.sigmoid(y_predictions_s))\n",
    "\n",
    "    #So we calculate something additional, the percentage of correct predictions \n",
    "    train_loss = loss_function(y_predictions_s, y_train)\n",
    "    percentage_correct = Model_Classification.accuracy(y_predictions, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    #Testing\n",
    "    Model_Classification.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        y_test_predictions_s = Model_Classification(x_test).squeeze()\n",
    "        y_test_predictions = torch.round(torch.sigmoid(y_test_predictions_s))\n",
    "\n",
    "        test_loss = loss_function(y_test_predictions_s, y_test)\n",
    "        test_percentage_correct = Model_Classification.accuracy(y_test_predictions, y_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    #Feedback\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {train_loss:.5}, Accuracy: {percentage_correct}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about what this code is doing, just know it helps us visualize why learning sucks right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)\n",
    "\n",
    "from helper_functions import plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(Model_Classification, x_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(Model_Classification, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Improving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what can we do when our model isn't performing how we like it?\n",
    "\n",
    "1. More Training\n",
    "2. More hidden layers & neurons\n",
    "3. Change Optimizers\n",
    "4. Change Activation Functions\n",
    "5. Alter Hyper parameters (learning rate, regularization rate etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network can mimic any function in any dimension, as long as you give it enough variables to mimic the function\n",
    "\n",
    "But why mimic functions? It's finding patterns to solving problems practically, in a way that we cannot understand<br>\n",
    "There's linear functions, and non linear function in a neural network\n",
    "\n",
    "For more please refer to http://neuralnetworksanddeeplearning.com/chap4.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lv2 Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following changes will be made for improving performance on the data<br>\n",
    "The addition of RELU will allow the model to handle non linear data\n",
    "\n",
    "1. More Neurons Per Hidden Layer `5 -> 10`\n",
    "2. More Hidden Layers `2 -> 4`\n",
    "3. Every Hidden layer Add `RELU` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification_lv2(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "        self.ReLU = nn.ReLU()\n",
    "        \n",
    "\n",
    "\n",
    "    #Later down the road, we can throw all this in a for loop, with all the layers in an iterable list\n",
    "    #Because right now manually doing a forward pass, and defining what layers use which activation functions... kinda dumb\n",
    "    def forward(self, x):\n",
    "        return self.layer_3(self.ReLU(self.layer_2(self.ReLU(self.layer_1(x)))))\n",
    "    \n",
    "\n",
    "    def accuracy(self, predictions, labeled_data):\n",
    "        correct = torch.eq(predictions, labeled_data).sum().item()\n",
    "        total_samples = len(labeled_data)\n",
    "        percentage_correct = (correct/total_samples) * 100\n",
    "        return percentage_correct\n",
    "    \n",
    "\n",
    "\n",
    "#Throw model to target device\n",
    "Model_Classification_Upgrade = Classification_lv2().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting loss function and optimizer\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(params=Model_Classification_Upgrade.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training/Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually it's the same code lol, except we train 2000 epochs and not 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "#Sending to cuda device\n",
    "x_test, x_train, y_test, y_train = x_test.to(device), x_train.to(device), y_test.to(device), y_train.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\n",
    "    #Training\n",
    "    Model_Classification_Upgrade.train()\n",
    "\n",
    "    #1. Forward Pass, round the output predictions\n",
    "    y_predictions_s = Model_Classification_Upgrade(x_train).squeeze()\n",
    "    y_predictions = torch.round(torch.sigmoid(y_predictions_s))\n",
    "\n",
    "    #2. Calculate the loss, with something additional, the percentage of correct predictions \n",
    "    train_loss = loss_function(y_predictions_s, y_train)\n",
    "    percentage_correct = Model_Classification_Upgrade.accuracy(y_predictions, y_train)\n",
    "\n",
    "    #3. Zero Grad, Back Propagation, Gradient Descent\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    #Testing\n",
    "    Model_Classification_Upgrade.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        y_test_predictions_s = Model_Classification_Upgrade(x_test).squeeze()\n",
    "        y_test_predictions = torch.round(torch.sigmoid(y_test_predictions_s))\n",
    "\n",
    "        test_loss = loss_function(y_test_predictions_s, y_test)\n",
    "        test_percentage_correct = Model_Classification_Upgrade.accuracy(y_test_predictions, y_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    #Feedback\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f}, Train Accuracy: {percentage_correct:.5f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_percentage_correct:.5f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about what this code is doing, just know it helps us visualize why learning sucks right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)\n",
    "\n",
    "from helper_functions import plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(Model_Classification_Upgrade, x_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(Model_Classification_Upgrade, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Save/Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the same code for Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#Create directory\n",
    "model_path = Path(\"models\")\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#Create saving path, usually pytorch files are called \"pth\"\n",
    "model_name = \"pytorch_classification_model_lv2.pth\"\n",
    "model_save_path = model_path / model_name\n",
    "\n",
    "#Saving the state dict\n",
    "torch.save(obj=Classification_lv2.state_dict(),\n",
    "           f=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading\n",
    "\n",
    "#we'll need to create a new model and load the saved state_dict() into the new model\n",
    "cooler_model = Classification_lv2()\n",
    "\n",
    "#loading the saved state dict from the new model, with torch.load()\n",
    "cooler_model.load_state_dict(torch.load(f=model_save_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}